{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS M148 Project Check-in 1\n",
        "\n",
        "**Project**: Spotify Music Track Analysis and Classification\n",
        "\n",
        "**Date**: December 12, 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. Dataset Description\n",
        "2. Main Features and Justification\n",
        "3. Data Cleaning and Missingness Handling\n",
        "4. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import io\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Dataset Description\n",
        "\n",
        "### What Dataset Did We Choose?\n",
        "\n",
        "Our team has chosen the **Spotify Tracks Dataset** from Hugging Face, which is publicly available and contains comprehensive audio feature data for over 114,000 music tracks across multiple genres.\n",
        "\n",
        "**Dataset Source**: [Hugging Face - maharshipandya/spotify-tracks-dataset](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)\n",
        "\n",
        "### Dataset Overview:\n",
        "- **Total Tracks**: ~114,000 songs\n",
        "- **Features**: 21 columns including audio characteristics and metadata\n",
        "- **Genres**: 114 different music genres (rock, pop, classical, jazz, hip-hop, electronic, etc.)\n",
        "- **Audio Features**: Quantitative measurements from Spotify's Audio Analysis API\n",
        "\n",
        "### Why This Dataset?\n",
        "\n",
        "We selected this dataset for several compelling reasons:\n",
        "\n",
        "1. **Rich Feature Set**: The dataset includes detailed audio analysis metrics (danceability, energy, valence, tempo, etc.) that provide deep insights into musical characteristics.\n",
        "\n",
        "2. **Large Sample Size**: With 114,000+ tracks, we have sufficient data for robust machine learning models and statistical analysis.\n",
        "\n",
        "3. **Multi-Genre Coverage**: 114 distinct genres allow us to explore how different musical styles differ in their acoustic properties.\n",
        "\n",
        "4. **Real-World Application**: Music recommendation systems and playlist generation are valuable commercial applications, making this project practically relevant.\n",
        "\n",
        "5. **Balanced Classes**: Each genre contains approximately 1,000 tracks, providing good class balance for classification tasks.\n",
        "\n",
        "6. **Data Quality**: The dataset is sourced from Spotify's official API, ensuring professional-grade measurements.\n",
        "\n",
        "### Research Questions We Can Explore:\n",
        "- Can we predict a song's genre based on its audio features?\n",
        "- What features most strongly influence a song's danceability?\n",
        "- Are there clusters of similar songs across different genres?\n",
        "- How do audio characteristics vary across different music genres?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from Hugging Face\n",
        "url = \"https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset/resolve/main/dataset.csv\"\n",
        "print(\"Downloading Spotify dataset from Hugging Face...\")\n",
        "response = requests.get(url, timeout=30)\n",
        "response.raise_for_status()\n",
        "df_original = pd.read_csv(io.StringIO(response.text))\n",
        "print(\"✓ Dataset loaded successfully!\\n\")\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset shape: {df_original.shape}\")\n",
        "print(f\"Number of tracks: {df_original.shape[0]:,}\")\n",
        "print(f\"Number of features: {df_original.shape[1]}\\n\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df_original.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display column information\n",
        "print(\"Dataset Columns and Data Types:\\n\")\n",
        "print(df_original.dtypes)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Dataset Summary Statistics:\")\n",
        "df_original.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Main Features in the Data\n",
        "\n",
        "### Audio Feature Descriptions\n",
        "\n",
        "Our analysis focuses on the following audio features provided by Spotify's Audio Analysis API:\n",
        "\n",
        "#### 1. **Danceability** (0.0 - 1.0)\n",
        "- **Description**: Describes how suitable a track is for dancing based on tempo, rhythm stability, beat strength, and overall regularity\n",
        "- **Why Important**: Key predictor for playlist generation and music recommendation; strongly correlated with genre\n",
        "- **Use Case**: Identifying party/workout music vs. relaxation music\n",
        "\n",
        "#### 2. **Energy** (0.0 - 1.0)\n",
        "- **Description**: Measures intensity and activity. High energy tracks feel fast, loud, and noisy\n",
        "- **Why Important**: Distinguishes between calm classical music and energetic rock/electronic music\n",
        "- **Use Case**: Mood-based music selection\n",
        "\n",
        "#### 3. **Loudness** (typically -60 to 0 dB)\n",
        "- **Description**: Overall loudness of a track in decibels (dB)\n",
        "- **Why Important**: Reflects production quality and genre conventions (e.g., metal is typically louder than folk)\n",
        "- **Use Case**: Normalizing audio playback, genre classification\n",
        "\n",
        "#### 4. **Speechiness** (0.0 - 1.0)\n",
        "- **Description**: Detects the presence of spoken words. High values indicate podcasts, audiobooks, or rap\n",
        "- **Why Important**: Differentiates between instrumental music, sung music, and spoken content\n",
        "- **Use Case**: Filtering out non-music content, identifying rap/hip-hop\n",
        "\n",
        "#### 5. **Acousticness** (0.0 - 1.0)\n",
        "- **Description**: Confidence measure of whether the track is acoustic (non-electric)\n",
        "- **Why Important**: Distinguishes acoustic vs. electronic production styles\n",
        "- **Use Case**: Finding unplugged versions, classical music, folk music\n",
        "\n",
        "#### 6. **Instrumentalness** (0.0 - 1.0)\n",
        "- **Description**: Predicts whether a track contains no vocals. High values indicate instrumental tracks\n",
        "- **Why Important**: Identifies background music, classical pieces, instrumental jazz\n",
        "- **Use Case**: Creating focus/study playlists, finding instrumental versions\n",
        "\n",
        "#### 7. **Liveness** (0.0 - 1.0)\n",
        "- **Description**: Detects presence of an audience in the recording. High values indicate live performances\n",
        "- **Why Important**: Distinguishes studio recordings from live albums\n",
        "- **Use Case**: Finding concert recordings, live albums\n",
        "\n",
        "#### 8. **Valence** (0.0 - 1.0)\n",
        "- **Description**: Musical positiveness. High valence = happy/cheerful, low valence = sad/angry\n",
        "- **Why Important**: Mood classification and emotional content analysis\n",
        "- **Use Case**: Creating mood-based playlists (happy, sad, angry, relaxed)\n",
        "\n",
        "#### 9. **Tempo** (BPM)\n",
        "- **Description**: Overall estimated tempo in beats per minute (BPM)\n",
        "- **Why Important**: Fundamental rhythmic characteristic; varies significantly across genres\n",
        "- **Use Case**: Matching songs for DJ mixing, workout intensity matching\n",
        "\n",
        "#### 10. **Duration (ms)**\n",
        "- **Description**: Track length in milliseconds\n",
        "- **Why Important**: Genre conventions (pop songs ~3 mins, classical movements vary widely)\n",
        "- **Use Case**: Playlist time management, genre classification\n",
        "\n",
        "#### 11. **Track Genre**\n",
        "- **Description**: Categorical label indicating the music genre\n",
        "- **Why Important**: Our primary target variable for classification tasks\n",
        "- **Use Case**: Genre prediction, genre-based recommendation\n",
        "\n",
        "### Why These Features Matter for Our Analysis\n",
        "\n",
        "These features are crucial because:\n",
        "\n",
        "1. **Objective Measurements**: Unlike subjective labels, these are quantitative measurements from audio analysis\n",
        "2. **Genre Differentiation**: Different genres have distinct patterns (e.g., classical has high acousticness and instrumentalness, EDM has high energy and danceability)\n",
        "3. **Prediction Power**: These features can predict user preferences and song characteristics\n",
        "4. **Industry Standard**: Used by Spotify for their recommendation algorithms\n",
        "5. **Interdependence**: Features correlate in interesting ways (e.g., energy often correlates with loudness and tempo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the key audio features for analysis\n",
        "audio_features = [\n",
        "    'danceability', 'energy', 'loudness', 'speechiness', \n",
        "    'acousticness', 'instrumentalness', 'liveness', 'valence', \n",
        "    'tempo', 'duration_ms'\n",
        "]\n",
        "\n",
        "# Check which features are available in the dataset\n",
        "available_features = [f for f in audio_features if f in df_original.columns]\n",
        "print(f\"Audio features available in dataset: {len(available_features)}/{len(audio_features)}\\n\")\n",
        "print(\"Available features:\")\n",
        "for i, feature in enumerate(available_features, 1):\n",
        "    print(f\"  {i}. {feature}\")\n",
        "\n",
        "# Check if genre column exists\n",
        "genre_col = 'track_genre' if 'track_genre' in df_original.columns else 'genre'\n",
        "if genre_col in df_original.columns:\n",
        "    print(f\"\\n✓ Genre column found: '{genre_col}'\")\n",
        "    print(f\"  Number of unique genres: {df_original[genre_col].nunique()}\")\n",
        "else:\n",
        "    print(\"\\n✗ Genre column not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Data Cleaning and Missingness Handling\n",
        "\n",
        "### Overview of Data Cleaning Process\n",
        "\n",
        "Data cleaning is crucial for ensuring the quality and reliability of our analysis. Our cleaning process includes:\n",
        "\n",
        "1. **Handling Duplicate Records**: Removing exact duplicate tracks\n",
        "2. **Missing Value Analysis**: Identifying patterns in missing data\n",
        "3. **Missing Value Imputation**: Filling missing values using appropriate strategies\n",
        "4. **Outlier Detection**: Identifying extreme values that may indicate data quality issues\n",
        "5. **Data Type Validation**: Ensuring all features have correct data types\n",
        "\n",
        "### Imputation Strategies\n",
        "\n",
        "We will demonstrate three common imputation techniques:\n",
        "\n",
        "1. **Mean Imputation**: Replace missing values with the feature mean (simple but ignores relationships)\n",
        "2. **Median Imputation**: Replace with median (robust to outliers)\n",
        "3. **KNN Imputation**: Use K-Nearest Neighbors to impute based on similar tracks (preserves relationships)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a working copy of the dataset\n",
        "df = df_original.copy()\n",
        "\n",
        "print(\"STEP 1: Initial Data Quality Assessment\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original dataset shape: {df.shape}\\n\")\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"✓ Removed {duplicates} duplicate rows\")\n",
        "    print(f\"New dataset shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"✓ No duplicate rows found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Analyze missing values\n",
        "print(\"\\nSTEP 2: Missing Value Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate missing values\n",
        "missing_counts = df.isnull().sum()\n",
        "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "# Create missing value summary\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Column': missing_counts.index,\n",
        "    'Missing Count': missing_counts.values,\n",
        "    'Missing %': missing_percentages.values\n",
        "})\n",
        "missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_summary) > 0:\n",
        "    print(\"\\nColumns with missing values:\\n\")\n",
        "    print(missing_summary.to_string(index=False))\n",
        "else:\n",
        "    print(\"\\n✓ No missing values found in the dataset!\")\n",
        "    print(\"\\nTo demonstrate data cleaning techniques, we'll artificially introduce\")\n",
        "    print(\"some missing values in the audio features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Introduce missing values for demonstration\n",
        "# (If the dataset already has missing values, skip this step)\n",
        "\n",
        "print(\"\\nSTEP 3: Introducing Missing Values for Demonstration\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a copy with artificial missing values\n",
        "df_with_missing = df.copy()\n",
        "np.random.seed(42)\n",
        "\n",
        "# Introduce ~5% missing values in selected audio features\n",
        "missing_rate = 0.05\n",
        "features_to_modify = ['energy', 'loudness', 'acousticness', 'valence', 'tempo']\n",
        "\n",
        "for feature in features_to_modify:\n",
        "    if feature in df_with_missing.columns:\n",
        "        # Randomly select indices to set as missing\n",
        "        n_missing = int(len(df_with_missing) * missing_rate)\n",
        "        missing_indices = np.random.choice(df_with_missing.index, size=n_missing, replace=False)\n",
        "        df_with_missing.loc[missing_indices, feature] = np.nan\n",
        "        print(f\"✓ Introduced {n_missing} missing values in '{feature}'\")\n",
        "\n",
        "# Show updated missing value summary\n",
        "print(\"\\nMissing value summary after introduction:\")\n",
        "missing_counts = df_with_missing[features_to_modify].isnull().sum()\n",
        "missing_percentages = (df_with_missing[features_to_modify].isnull().sum() / len(df_with_missing)) * 100\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Feature': missing_counts.index,\n",
        "    'Missing Count': missing_counts.values,\n",
        "    'Missing %': missing_percentages.values\n",
        "})\n",
        "print(missing_summary.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize missing data patterns\n",
        "print(\"\\nVisualizing Missing Data Patterns:\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Missing value counts\n",
        "missing_data = df_with_missing[features_to_modify].isnull().sum().sort_values(ascending=False)\n",
        "axes[0].bar(range(len(missing_data)), missing_data.values, color='coral')\n",
        "axes[0].set_xticks(range(len(missing_data)))\n",
        "axes[0].set_xticklabels(missing_data.index, rotation=45, ha='right')\n",
        "axes[0].set_ylabel('Number of Missing Values')\n",
        "axes[0].set_title('Missing Values by Feature')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Missing value percentages\n",
        "missing_pct = (df_with_missing[features_to_modify].isnull().sum() / len(df_with_missing) * 100).sort_values(ascending=False)\n",
        "axes[1].bar(range(len(missing_pct)), missing_pct.values, color='skyblue')\n",
        "axes[1].set_xticks(range(len(missing_pct)))\n",
        "axes[1].set_xticklabels(missing_pct.index, rotation=45, ha='right')\n",
        "axes[1].set_ylabel('Percentage of Missing Values (%)')\n",
        "axes[1].set_title('Missing Values Percentage by Feature')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].axhline(y=5, color='red', linestyle='--', linewidth=1, label='5% threshold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: Imputation Method 1 - Mean Imputation\n",
        "print(\"\\nSTEP 4: Imputation Method 1 - Mean Imputation\")\n",
        "print(\"=\"*60)\n",
        "print(\"Strategy: Replace missing values with the mean of each feature\")\n",
        "print(\"Pros: Simple, fast, preserves overall mean\")\n",
        "print(\"Cons: Reduces variance, ignores relationships between features\\n\")\n",
        "\n",
        "# Create copy for mean imputation\n",
        "df_mean_imputed = df_with_missing.copy()\n",
        "\n",
        "# Apply mean imputation\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "df_mean_imputed[features_to_modify] = imputer_mean.fit_transform(df_mean_imputed[features_to_modify])\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df_mean_imputed[features_to_modify].isnull().sum().sum()\n",
        "print(f\"✓ Mean imputation completed\")\n",
        "print(f\"✓ Remaining missing values: {remaining_missing}\")\n",
        "\n",
        "# Show example of imputed values\n",
        "print(\"\\nExample: Mean values used for imputation:\")\n",
        "for feature in features_to_modify:\n",
        "    mean_val = imputer_mean.statistics_[features_to_modify.index(feature)]\n",
        "    print(f\"  {feature}: {mean_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: Imputation Method 2 - Median Imputation\n",
        "print(\"\\nSTEP 5: Imputation Method 2 - Median Imputation\")\n",
        "print(\"=\"*60)\n",
        "print(\"Strategy: Replace missing values with the median of each feature\")\n",
        "print(\"Pros: Robust to outliers, preserves central tendency\")\n",
        "print(\"Cons: Still ignores relationships between features\\n\")\n",
        "\n",
        "# Create copy for median imputation\n",
        "df_median_imputed = df_with_missing.copy()\n",
        "\n",
        "# Apply median imputation\n",
        "imputer_median = SimpleImputer(strategy='median')\n",
        "df_median_imputed[features_to_modify] = imputer_median.fit_transform(df_median_imputed[features_to_modify])\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df_median_imputed[features_to_modify].isnull().sum().sum()\n",
        "print(f\"✓ Median imputation completed\")\n",
        "print(f\"✓ Remaining missing values: {remaining_missing}\")\n",
        "\n",
        "# Show example of imputed values\n",
        "print(\"\\nExample: Median values used for imputation:\")\n",
        "for feature in features_to_modify:\n",
        "    median_val = imputer_median.statistics_[features_to_modify.index(feature)]\n",
        "    print(f\"  {feature}: {median_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6: Imputation Method 3 - KNN Imputation\n",
        "print(\"\\nSTEP 6: Imputation Method 3 - KNN Imputation\")\n",
        "print(\"=\"*60)\n",
        "print(\"Strategy: Impute using K-Nearest Neighbors based on similar tracks\")\n",
        "print(\"Pros: Preserves relationships between features, more accurate\")\n",
        "print(\"Cons: Computationally expensive, requires more memory\\n\")\n",
        "\n",
        "# Create copy for KNN imputation\n",
        "df_knn_imputed = df_with_missing.copy()\n",
        "\n",
        "# Apply KNN imputation (using k=5 neighbors)\n",
        "print(\"Performing KNN imputation (k=5 neighbors)...\")\n",
        "imputer_knn = KNNImputer(n_neighbors=5)\n",
        "df_knn_imputed[features_to_modify] = imputer_knn.fit_transform(df_knn_imputed[features_to_modify])\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = df_knn_imputed[features_to_modify].isnull().sum().sum()\n",
        "print(f\"✓ KNN imputation completed\")\n",
        "print(f\"✓ Remaining missing values: {remaining_missing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 7: Compare imputation methods\n",
        "print(\"\\nSTEP 7: Comparing Imputation Methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate statistics for each method\n",
        "comparison_data = []\n",
        "\n",
        "for feature in features_to_modify:\n",
        "    original_mean = df[feature].mean()\n",
        "    original_std = df[feature].std()\n",
        "    \n",
        "    mean_imputed_mean = df_mean_imputed[feature].mean()\n",
        "    mean_imputed_std = df_mean_imputed[feature].std()\n",
        "    \n",
        "    median_imputed_mean = df_median_imputed[feature].mean()\n",
        "    median_imputed_std = df_median_imputed[feature].std()\n",
        "    \n",
        "    knn_imputed_mean = df_knn_imputed[feature].mean()\n",
        "    knn_imputed_std = df_knn_imputed[feature].std()\n",
        "    \n",
        "    comparison_data.append({\n",
        "        'Feature': feature,\n",
        "        'Original Mean': original_mean,\n",
        "        'Mean Imp. Mean': mean_imputed_mean,\n",
        "        'Median Imp. Mean': median_imputed_mean,\n",
        "        'KNN Imp. Mean': knn_imputed_mean,\n",
        "        'Original Std': original_std,\n",
        "        'Mean Imp. Std': mean_imputed_std,\n",
        "        'Median Imp. Std': median_imputed_std,\n",
        "        'KNN Imp. Std': knn_imputed_std\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nComparison of Imputation Methods:\")\n",
        "print(\"\\nMean Comparison:\")\n",
        "print(comparison_df[['Feature', 'Original Mean', 'Mean Imp. Mean', 'Median Imp. Mean', 'KNN Imp. Mean']].to_string(index=False))\n",
        "print(\"\\nStandard Deviation Comparison:\")\n",
        "print(comparison_df[['Feature', 'Original Std', 'Mean Imp. Std', 'Median Imp. Std', 'KNN Imp. Std']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the impact of different imputation methods\n",
        "print(\"\\nVisualizing Imputation Method Comparison:\\n\")\n",
        "\n",
        "# Select one feature for detailed comparison\n",
        "example_feature = 'energy'\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Original distribution\n",
        "axes[0, 0].hist(df[example_feature], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "axes[0, 0].set_title(f'Original {example_feature.title()} Distribution')\n",
        "axes[0, 0].set_xlabel(example_feature.title())\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df[example_feature].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Mean imputation\n",
        "axes[0, 1].hist(df_mean_imputed[example_feature], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[0, 1].set_title(f'Mean Imputed {example_feature.title()} Distribution')\n",
        "axes[0, 1].set_xlabel(example_feature.title())\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].axvline(df_mean_imputed[example_feature].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Median imputation\n",
        "axes[1, 0].hist(df_median_imputed[example_feature], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
        "axes[1, 0].set_title(f'Median Imputed {example_feature.title()} Distribution')\n",
        "axes[1, 0].set_xlabel(example_feature.title())\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].axvline(df_median_imputed[example_feature].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# KNN imputation\n",
        "axes[1, 1].hist(df_knn_imputed[example_feature], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].set_title(f'KNN Imputed {example_feature.title()} Distribution')\n",
        "axes[1, 1].set_xlabel(example_feature.title())\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].axvline(df_knn_imputed[example_feature].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Mean/Median imputation may create artificial peaks at the mean/median value\")\n",
        "print(\"- KNN imputation better preserves the original distribution shape\")\n",
        "print(\"- KNN imputation considers relationships between features for more accurate estimates\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
