{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display and plotting defaults\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset from Hugging Face...\n",
            "Dataset loaded successfully!\n",
            "\n",
            "Dataset shape: (114000, 21)\n",
            "Number of columns: 21\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>track_id</th>\n",
              "      <th>artists</th>\n",
              "      <th>album_name</th>\n",
              "      <th>track_name</th>\n",
              "      <th>popularity</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>explicit</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>valence</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>track_genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
              "      <td>Gen Hoshino</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>73</td>\n",
              "      <td>230666</td>\n",
              "      <td>False</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.4610</td>\n",
              "      <td>1</td>\n",
              "      <td>-6.746</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1430</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.3580</td>\n",
              "      <td>0.715</td>\n",
              "      <td>87.917</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
              "      <td>Ben Woodward</td>\n",
              "      <td>Ghost (Acoustic)</td>\n",
              "      <td>Ghost - Acoustic</td>\n",
              "      <td>55</td>\n",
              "      <td>149610</td>\n",
              "      <td>False</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.1660</td>\n",
              "      <td>1</td>\n",
              "      <td>-17.235</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0763</td>\n",
              "      <td>0.9240</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.267</td>\n",
              "      <td>77.489</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
              "      <td>Ingrid Michaelson;ZAYN</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>To Begin Again</td>\n",
              "      <td>57</td>\n",
              "      <td>210826</td>\n",
              "      <td>False</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.3590</td>\n",
              "      <td>0</td>\n",
              "      <td>-9.734</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0557</td>\n",
              "      <td>0.2100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1170</td>\n",
              "      <td>0.120</td>\n",
              "      <td>76.332</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
              "      <td>Kina Grannis</td>\n",
              "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
              "      <td>Can't Help Falling In Love</td>\n",
              "      <td>71</td>\n",
              "      <td>201933</td>\n",
              "      <td>False</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.0596</td>\n",
              "      <td>0</td>\n",
              "      <td>-18.515</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.9050</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.1320</td>\n",
              "      <td>0.143</td>\n",
              "      <td>181.740</td>\n",
              "      <td>3</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
              "      <td>Chord Overstreet</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>Hold On</td>\n",
              "      <td>82</td>\n",
              "      <td>198853</td>\n",
              "      <td>False</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.4430</td>\n",
              "      <td>2</td>\n",
              "      <td>-9.681</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0526</td>\n",
              "      <td>0.4690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0829</td>\n",
              "      <td>0.167</td>\n",
              "      <td>119.949</td>\n",
              "      <td>4</td>\n",
              "      <td>acoustic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                track_id                 artists  \\\n",
              "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
              "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
              "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
              "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
              "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
              "\n",
              "                                          album_name  \\\n",
              "0                                             Comedy   \n",
              "1                                   Ghost (Acoustic)   \n",
              "2                                     To Begin Again   \n",
              "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
              "4                                            Hold On   \n",
              "\n",
              "                   track_name  popularity  duration_ms  explicit  \\\n",
              "0                      Comedy          73       230666     False   \n",
              "1            Ghost - Acoustic          55       149610     False   \n",
              "2              To Begin Again          57       210826     False   \n",
              "3  Can't Help Falling In Love          71       201933     False   \n",
              "4                     Hold On          82       198853     False   \n",
              "\n",
              "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
              "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
              "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
              "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
              "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
              "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
              "\n",
              "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
              "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
              "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
              "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
              "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
              "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load dataset from Hugging Face - direct CSV download\n",
        "import requests\n",
        "import io\n",
        "\n",
        "url = \"https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset/resolve/main/dataset.csv\"\n",
        "print(\"Downloading dataset from Hugging Face...\")\n",
        "response = requests.get(url, timeout=30)\n",
        "response.raise_for_status()\n",
        "df = pd.read_csv(io.StringIO(response.text))\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# Basic cleaning: drop obvious duplicates and reset index\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features used: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
            "Data shape: (114000, 10)\n",
            "\n",
            "Number of unique genres: 114\n",
            "\n",
            "Genre distribution (top 20):\n",
            "track_genre\n",
            "acoustic             1000\n",
            "punk-rock            1000\n",
            "progressive-house    1000\n",
            "power-pop            1000\n",
            "pop                  1000\n",
            "pop-film             1000\n",
            "piano                1000\n",
            "party                1000\n",
            "pagode               1000\n",
            "opera                1000\n",
            "new-age              1000\n",
            "mpb                  1000\n",
            "minimal-techno       1000\n",
            "metalcore            1000\n",
            "metal                1000\n",
            "mandopop             1000\n",
            "malay                1000\n",
            "latino               1000\n",
            "latin                1000\n",
            "kids                 1000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "All genres: ['acoustic', 'afrobeat', 'alt-rock', 'alternative', 'ambient', 'anime', 'black-metal', 'bluegrass', 'blues', 'brazil', 'breakbeat', 'british', 'cantopop', 'chicago-house', 'children', 'chill', 'classical', 'club', 'comedy', 'country', 'dance', 'dancehall', 'death-metal', 'deep-house', 'detroit-techno', 'disco', 'disney', 'drum-and-bass', 'dub', 'dubstep', 'edm', 'electro', 'electronic', 'emo', 'folk', 'forro', 'french', 'funk', 'garage', 'german', 'gospel', 'goth', 'grindcore', 'groove', 'grunge', 'guitar', 'happy', 'hard-rock', 'hardcore', 'hardstyle', 'heavy-metal', 'hip-hop', 'honky-tonk', 'house', 'idm', 'indian', 'indie', 'indie-pop', 'industrial', 'iranian', 'j-dance', 'j-idol', 'j-pop', 'j-rock', 'jazz', 'k-pop', 'kids', 'latin', 'latino', 'malay', 'mandopop', 'metal', 'metalcore', 'minimal-techno', 'mpb', 'new-age', 'opera', 'pagode', 'party', 'piano', 'pop', 'pop-film', 'power-pop', 'progressive-house', 'psych-rock', 'punk', 'punk-rock', 'r-n-b', 'reggae', 'reggaeton', 'rock', 'rock-n-roll', 'rockabilly', 'romance', 'sad', 'salsa', 'samba', 'sertanejo', 'show-tunes', 'singer-songwriter', 'ska', 'sleep', 'songwriter', 'soul', 'spanish', 'study', 'swedish', 'synth-pop', 'tango', 'techno', 'trance', 'trip-hop', 'turkish', 'world-music']\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for neural network\n",
        "# Select audio features as predictors (exclude danceability since it's the target)\n",
        "num_features = [\n",
        "    'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms'\n",
        "]\n",
        "\n",
        "available = [c for c in num_features if c in df.columns]\n",
        "\n",
        "# Keep rows with no NaNs in used columns and danceability (target)\n",
        "model_df = df.dropna(subset=available + ['danceability']).copy()\n",
        "\n",
        "# Inputs: audio features; Target: danceability continuous\n",
        "X = model_df[available].values\n",
        "y_continuous = model_df['danceability'].values\n",
        "\n",
        "print(f\"Features used: {available}\")\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"Danceability stats (full data): min={y_continuous.min():.3f}, max={y_continuous.max():.3f}, median={np.median(y_continuous):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Convert to categorical (one-hot encoding) for neural network\u001b[39;00m\n\u001b[1;32m     16\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(le\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m---> 17\u001b[0m y_train_cat \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_train, num_classes)\n\u001b[1;32m     18\u001b[0m y_val_cat \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_val, num_classes)\n\u001b[1;32m     19\u001b[0m y_test_cat \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(y_test, num_classes)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "# Split data into train, validation, and test sets\n",
        "X_train, X_temp, y_train_cont, y_temp_cont = train_test_split(\n",
        "    X, y_continuous, test_size=0.3, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val_cont, y_test_cont = train_test_split(\n",
        "    X_temp, y_temp_cont, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Compute median on training set only (to avoid leakage)\n",
        "median_dance = np.median(y_train_cont)\n",
        "print(f\"Training-set median danceability: {median_dance:.3f}\")\n",
        "\n",
        "# Create binary labels: 1 if >= median, 0 otherwise\n",
        "y_train = (y_train_cont >= median_dance).astype(int)\n",
        "y_val = (y_val_cont >= median_dance).astype(int)\n",
        "y_test = (y_test_cont >= median_dance).astype(int)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to categorical (one-hot encoding) for neural network\n",
        "num_classes = 2\n",
        "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val_cat = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val_scaled.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_scaled.shape[0]} samples\")\n",
        "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Building the Neural Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create neural network model\n",
        "def create_model(learning_rate=0.001, hidden_units=64, dropout_rate=0.3):\n",
        "    \"\"\"\n",
        "    Create a feedforward neural network for binary classification (danceability >= median vs < median).\n",
        "    \n",
        "    Parameters:\n",
        "    - learning_rate: Learning rate for optimizer\n",
        "    - hidden_units: Number of units in hidden layers\n",
        "    - dropout_rate: Dropout rate for regularization\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(hidden_units, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(hidden_units, activation='relu'),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(hidden_units // 2, activation='relu'),\n",
        "        layers.Dropout(dropout_rate / 2),\n",
        "        layers.Dense(num_classes, activation='softmax')  # num_classes=2\n",
        "    ])\n",
        "    \n",
        "    # Compile model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create initial model to show architecture\n",
        "initial_model = create_model(learning_rate=0.001, hidden_units=64, dropout_rate=0.3)\n",
        "print(\"Neural Network Architecture:\")\n",
        "initial_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Hyperparameter Tuning - Learning Rate Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning: Learning rate search (FAST)\n",
        "# Goal: ~4x faster by using only 10,000 training samples + fewer learning rates + fewer epochs\n",
        "\n",
        "sample_size = min(10000, len(X_train_scaled))\n",
        "if sample_size < len(X_train_scaled):\n",
        "    print(f\"Using sample of {sample_size} training samples for faster hyperparameter search...\")\n",
        "    np.random.seed(42)\n",
        "    sample_indices = np.random.choice(len(X_train_scaled), size=sample_size, replace=False)\n",
        "    X_train_sample = X_train_scaled[sample_indices]\n",
        "    y_train_sample = y_train_cat[sample_indices]\n",
        "else:\n",
        "    X_train_sample = X_train_scaled\n",
        "    y_train_sample = y_train_cat\n",
        "\n",
        "# Test fewer learning rates (reduces total models trained)\n",
        "learning_rates = [0.001, 0.01]\n",
        "lr_results = []\n",
        "\n",
        "print(\"\\nTesting different learning rates...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"(Fast mode: 10k samples, <=10 epochs per candidate)\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTesting learning rate: {lr}\")\n",
        "\n",
        "    # Create model with current learning rate\n",
        "    model = create_model(learning_rate=lr, hidden_units=64, dropout_rate=0.3)\n",
        "\n",
        "    # Early stopping to prevent overfitting (aggressive for speed)\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train model (reduced epochs for speed)\n",
        "    history = model.fit(\n",
        "        X_train_sample, y_train_sample,\n",
        "        validation_data=(X_val_scaled, y_val_cat),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Get best validation accuracy\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    best_train_acc = max(history.history['accuracy'])\n",
        "    epochs_trained = len(history.history['loss'])\n",
        "    \n",
        "    lr_results.append({\n",
        "        'learning_rate': lr,\n",
        "        'val_accuracy': best_val_acc,\n",
        "        'train_accuracy': best_train_acc,\n",
        "        'epochs': epochs_trained\n",
        "    })\n",
        "    \n",
        "    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"  Epochs trained: {epochs_trained}\")\n",
        "\n",
        "# Display results\n",
        "lr_df = pd.DataFrame(lr_results)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Learning Rate Search Results:\")\n",
        "print(lr_df.to_string(index=False))\n",
        "\n",
        "# Find optimal learning rate\n",
        "optimal_lr = lr_df.loc[lr_df['val_accuracy'].idxmax(), 'learning_rate']\n",
        "print(f\"\\nOptimal learning rate: {optimal_lr} (validation accuracy: {lr_df.loc[lr_df['val_accuracy'].idxmax(), 'val_accuracy']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize learning rate search results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(lr_df['learning_rate'], lr_df['val_accuracy'], marker='o', label='Validation', linewidth=2)\n",
        "axes[0].plot(lr_df['learning_rate'], lr_df['train_accuracy'], marker='s', label='Training', linewidth=2)\n",
        "axes[0].axvline(optimal_lr, color='r', linestyle='--', label=f'Optimal LR: {optimal_lr}')\n",
        "axes[0].set_xlabel('Learning Rate')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Learning Rate vs Accuracy')\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(range(len(lr_df)), lr_df['val_accuracy'], color=['red' if lr == optimal_lr else 'blue' for lr in lr_df['learning_rate']])\n",
        "axes[1].set_xlabel('Learning Rate Index')\n",
        "axes[1].set_ylabel('Validation Accuracy')\n",
        "axes[1].set_title('Validation Accuracy by Learning Rate')\n",
        "axes[1].set_xticks(range(len(lr_df)))\n",
        "axes[1].set_xticklabels([f\"{lr:.4f}\" for lr in lr_df['learning_rate']], rotation=45)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Training Final Model with Optimal Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model with optimal learning rate (FAST)\n",
        "# Goal: ~4x faster by using only 10,000 training samples + fewer epochs\n",
        "print(f\"Training final model with optimal learning rate: {optimal_lr}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure we have a 10,000-sample training subset (in case you skipped the LR-search cell)\n",
        "if 'X_train_sample' not in globals() or 'y_train_sample' not in globals():\n",
        "    sample_size = min(10000, len(X_train_scaled))\n",
        "    np.random.seed(42)\n",
        "    sample_indices = np.random.choice(len(X_train_scaled), size=sample_size, replace=False)\n",
        "    X_train_sample = X_train_scaled[sample_indices]\n",
        "    y_train_sample = y_train_cat[sample_indices]\n",
        "\n",
        "print(f\"Using {len(X_train_sample)} training samples for final training (fast mode)\")\n",
        "\n",
        "# Create final model\n",
        "final_model = create_model(learning_rate=optimal_lr, hidden_units=64, dropout_rate=0.3)\n",
        "\n",
        "# Callbacks for training\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = final_model.fit(\n",
        "    X_train_sample, y_train_sample,\n",
        "    validation_data=(X_val_scaled, y_val_cat),\n",
        "    epochs=25,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Accuracy During Training')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Model Loss During Training')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "print(f\"\\nFinal Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Model Evaluation and Performance Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = final_model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_proba = final_model.predict(X_test_scaled, verbose=0)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Classification report\n",
        "class_names = ['Below Median', 'Above Median']\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "class_names = ['Below Median', 'Above Median']\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy\n",
        "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
        "print(\"\\nPer-Class Accuracy:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"  {class_name}: {class_accuracies[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation: Neural Network Application\n",
        "\n",
        "### Task Description\n",
        "\n",
        "We built a **feedforward neural network** for **multi-class classification** to predict song **genre** based on audio features. This is a supervised learning task where:\n",
        "\n",
        "- **Input**: 10 audio features (danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms)\n",
        "- **Output**: Music genre (e.g., pop, rock, hip-hop, classical, electronic, etc.)\n",
        "- **Architecture**: Multi-layer perceptron with dropout regularization\n",
        "\n",
        "### Network Architecture\n",
        "\n",
        "The neural network consists of:\n",
        "1. **Input Layer**: 10 neurons (one per audio feature)\n",
        "2. **Hidden Layer 1**: 64 neurons with ReLU activation + Dropout (0.3)\n",
        "3. **Hidden Layer 2**: 64 neurons with ReLU activation + Dropout (0.3)\n",
        "4. **Hidden Layer 3**: 32 neurons with ReLU activation + Dropout (0.15)\n",
        "5. **Output Layer**: N neurons with Softmax activation (one per genre class)\n",
        "\n",
        "**Why this architecture?**\n",
        "- Multiple hidden layers allow the network to learn complex non-linear relationships between audio features and genre\n",
        "- Dropout layers prevent overfitting by randomly deactivating neurons during training\n",
        "- ReLU activation functions enable non-linear learning while being computationally efficient\n",
        "- Softmax output ensures probabilities sum to 1 for multi-class classification\n",
        "\n",
        "**Why genre prediction is meaningful:**\n",
        "- Different genres have distinct audio characteristics (e.g., classical has high acousticness, EDM has high energy)\n",
        "- The model learns these patterns from the audio features\n",
        "- This can be useful for music recommendation systems and playlist generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation: Performance Metrics\n",
        "\n",
        "### Metrics Used to Assess NN Performance\n",
        "\n",
        "We use multiple metrics to comprehensively evaluate the neural network:\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "- **Definition**: Proportion of correct predictions (TP + TN) / Total samples\n",
        "- **Why it matters**: Overall model performance across all genre classes\n",
        "- **Limitation**: Can be misleading with imbalanced classes\n",
        "\n",
        "#### 2. **Loss (Categorical Cross-Entropy)**\n",
        "- **Definition**: Measures the difference between predicted probabilities and true labels\n",
        "- **Why it matters**: Directly optimized during training; lower is better\n",
        "- **Interpretation**: Penalizes confident wrong predictions more heavily\n",
        "\n",
        "#### 3. **Per-Class Metrics** (from Classification Report)\n",
        "- **Precision**: TP / (TP + FP) - Of predicted genres, how many are correct?\n",
        "- **Recall**: TP / (TP + FN) - Of actual genres, how many were found?\n",
        "- **F1-Score**: Harmonic mean of precision and recall - Balanced metric\n",
        "- **Support**: Number of samples in each genre\n",
        "\n",
        "#### 4. **Confusion Matrix**\n",
        "- **Definition**: Table showing actual vs predicted classifications\n",
        "- **Why it matters**: Reveals which genres are confused with each other\n",
        "- **Insight**: Helps identify systematic misclassification patterns (e.g., confusing rock with metal)\n",
        "\n",
        "**Why these metrics?**\n",
        "- **Accuracy** gives overall performance but can hide genre-specific issues\n",
        "- **Per-class metrics** reveal if the model performs well for all genres\n",
        "- **Confusion matrix** shows specific error patterns (e.g., confusing similar genres)\n",
        "- Together, they provide a complete picture of model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation: Training Process and Hyperparameter Learning\n",
        "\n",
        "### How the Neural Network Was Trained\n",
        "\n",
        "#### 1. **Data Preparation**\n",
        "- **Train/Validation/Test Split**: 70%/15%/15% split with stratification to maintain class distribution\n",
        "- **Feature Standardization**: All features scaled to mean=0, std=1 using StandardScaler\n",
        "- **One-Hot Encoding**: Labels converted to categorical format for multi-class classification\n",
        "\n",
        "#### 2. **Hyperparameter Tuning: Learning Rate**\n",
        "\n",
        "**Method**: Grid search over learning rates [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "**Process**:\n",
        "- Trained separate models with each learning rate\n",
        "- Used early stopping (patience=5) to prevent overfitting\n",
        "- Evaluated on validation set to find optimal value\n",
        "- Selected learning rate with highest validation accuracy\n",
        "\n",
        "**Results**: The optimal learning rate balances:\n",
        "- **Too low** (0.0001): Slow convergence, may not reach optimal solution\n",
        "- **Too high** (0.1): Unstable training, may overshoot optimal weights\n",
        "- **Optimal** (typically 0.001-0.01): Fast convergence with stable training\n",
        "\n",
        "#### 3. **Training Configuration**\n",
        "\n",
        "**Optimizer**: Adam (Adaptive Moment Estimation)\n",
        "- Combines benefits of momentum and adaptive learning rates\n",
        "- Automatically adjusts learning rate per parameter\n",
        "\n",
        "**Batch Size**: 32\n",
        "- Processes 32 samples per gradient update\n",
        "- Balances memory usage and gradient stability\n",
        "\n",
        "**Epochs**: Up to 100 (with early stopping)\n",
        "- Early stopping monitors validation loss\n",
        "- Stops training if no improvement for 10 epochs\n",
        "- Restores best weights to prevent overfitting\n",
        "\n",
        "**Callbacks Used**:\n",
        "- **EarlyStopping**: Prevents overfitting by stopping when validation loss stops improving\n",
        "- **ReduceLROnPlateau**: Dynamically reduces learning rate if validation loss plateaus\n",
        "\n",
        "#### 4. **Regularization Techniques**\n",
        "\n",
        "- **Dropout**: Randomly deactivates 30% of neurons during training to prevent overfitting\n",
        "- **Validation Set**: Used to monitor generalization performance during training\n",
        "- **Early Stopping**: Prevents training beyond optimal point\n",
        "\n",
        "### Hyperparameter Learning Process\n",
        "\n",
        "1. **Initial Exploration**: Tested learning rates across orders of magnitude (0.0001 to 0.1)\n",
        "2. **Validation-Based Selection**: Chose learning rate with best validation performance\n",
        "3. **Final Training**: Trained final model with optimal hyperparameters on full training set\n",
        "4. **Test Evaluation**: Final performance assessed on held-out test set\n",
        "\n",
        "**Key Insight**: Learning rate is critical - too high causes instability, too low causes slow convergence. The grid search identified the sweet spot for this dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
