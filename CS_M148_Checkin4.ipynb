{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS M148 Project Check-In 4\n",
    "\n",
    "**Due Date:** November 7, 2025 at 11:59 P.M.\n",
    "\n",
    "This notebook documents progress for the classification check-in:\n",
    "\n",
    "1. Apply KNN algorithm or Random Forest Algorithm for classification on a binary categorical response variable\n",
    "2. Calculate confusion matrix, prediction accuracy, prediction error, true positive rate, true negative rate, and F1 score on training data\n",
    "3. Calculate and plot ROC curve and AUC on validation data\n",
    "4. Use 5-fold cross-validation on validation set to calculate AUC and accuracy of each fold\n",
    "\n",
    "**Dataset:** Spotify Tracks (Hugging Face) â€” `maharshipandya/spotify-tracks-dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m ds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m df \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Basic cleaning: drop obvious duplicates and reset index\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, classification_report,\n",
    "    roc_curve, roc_auc_score, f1_score, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Display and plotting defaults\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "print(\"Loading dataset...\")\n",
    "ds = pd.read_csv(\"hf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv\")\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "# Basic cleaning: drop obvious duplicates and reset index\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Binary Categorical Response Variable\n",
    "\n",
    "We'll create a binary classification task by predicting whether a track is **popular** or not.\n",
    "We'll define \"popular\" as having a popularity score >= 50 (the median/threshold).\n",
    "\n",
    "Alternatively, we could use `explicit` (True/False) if available in the dataset, or create other meaningful binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'popularity' column exists and examine its distribution\n",
    "if 'popularity' in df.columns:\n",
    "    print(\"Popularity statistics:\")\n",
    "    print(df['popularity'].describe())\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df['popularity'], kde=True, bins=50)\n",
    "    plt.title('Distribution of Popularity')\n",
    "    plt.xlabel('Popularity Score')\n",
    "    \n",
    "    # Create binary target: popular = 1 if popularity >= 50, else 0\n",
    "    threshold = 50\n",
    "    df['is_popular'] = (df['popularity'] >= threshold).astype(int)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df['is_popular'].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Binary Classification Target (threshold={threshold})')\n",
    "    plt.xlabel('Is Popular')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Not Popular (0)', 'Popular (1)'], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['is_popular'].value_counts())\n",
    "    print(f\"\\nClass balance: {df['is_popular'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for classification\n",
    "# Use audio features similar to Check-In 2\n",
    "num_features = [\n",
    "    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
    "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms'\n",
    "]\n",
    "\n",
    "available = [c for c in num_features if c in df.columns]\n",
    "missing = sorted(set(num_features) - set(available))\n",
    "if missing:\n",
    "    print('Missing columns skipped:', missing)\n",
    "\n",
    "print(f\"Using features: {available}\")\n",
    "\n",
    "# Keep rows with no NaNs in used columns\n",
    "model_df = df.dropna(subset=available + ['is_popular']).copy()\n",
    "\n",
    "X = model_df[available]\n",
    "y = model_df['is_popular']\n",
    "\n",
    "# Split into train and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"\\nTrain class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nValidation class distribution:\\n{y_val.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply KNN Algorithm for Classification\n",
    "\n",
    "We'll use K-Nearest Neighbors (KNN) with different values of k and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KNN pipeline with standardization\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "# Fit KNN on training data\n",
    "print(\"Training KNN classifier...\")\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on training and validation sets\n",
    "y_train_pred_knn = knn_pipeline.predict(X_train)\n",
    "y_val_pred_knn = knn_pipeline.predict(X_val)\n",
    "\n",
    "# Prediction probabilities for ROC curve\n",
    "y_train_proba_knn = knn_pipeline.predict_proba(X_train)[:, 1]\n",
    "y_val_proba_knn = knn_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"KNN training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Random Forest Algorithm for Classification\n",
    "\n",
    "We'll also train a Random Forest classifier for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10))\n",
    "])\n",
    "\n",
    "# Fit Random Forest on training data\n",
    "print(\"Training Random Forest classifier...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on training and validation sets\n",
    "y_train_pred_rf = rf_pipeline.predict(X_train)\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "# Prediction probabilities for ROC curve\n",
    "y_train_proba_rf = rf_pipeline.predict_proba(X_train)[:, 1]\n",
    "y_val_proba_rf = rf_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Random Forest training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Metrics on Training Data\n",
    "\n",
    "We'll calculate:\n",
    "- Confusion Matrix\n",
    "- Prediction Accuracy\n",
    "- Prediction Error (1 - Accuracy)\n",
    "- True Positive Rate (Recall/Sensitivity)\n",
    "- True Negative Rate (Specificity)\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate and display classification metrics.\n",
    "    \"\"\"\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    error = 1 - accuracy\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate (Recall/Sensitivity)\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate (Specificity)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} - Metrics\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TN: {tn:6d}  |  FP: {fp:6d}\")\n",
    "    print(f\"  FN: {fn:6d}  |  TP: {tp:6d}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Accuracy:              {accuracy:.4f}\")\n",
    "    print(f\"  Prediction Error:      {error:.4f}\")\n",
    "    print(f\"  True Positive Rate:    {tpr:.4f}  (Sensitivity/Recall)\")\n",
    "    print(f\"  True Negative Rate:    {tnr:.4f}  (Specificity)\")\n",
    "    print(f\"  F1 Score:              {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'error': error,\n",
    "        'tpr': tpr,\n",
    "        'tnr': tnr,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Calculate metrics for KNN on training data\n",
    "knn_train_metrics = calculate_metrics(y_train, y_train_pred_knn, \"KNN (Training Set)\")\n",
    "\n",
    "# Calculate metrics for Random Forest on training data\n",
    "rf_train_metrics = calculate_metrics(y_train, y_train_pred_rf, \"Random Forest (Training Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices for training data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# KNN Confusion Matrix\n",
    "ConfusionMatrixDisplay(knn_train_metrics['confusion_matrix'], \n",
    "                       display_labels=['Not Popular', 'Popular']).plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('KNN - Confusion Matrix (Training Set)')\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "ConfusionMatrixDisplay(rf_train_metrics['confusion_matrix'], \n",
    "                       display_labels=['Not Popular', 'Popular']).plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Random Forest - Confusion Matrix (Training Set)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and Plot ROC Curve and AUC on Validation Data\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve shows the trade-off between True Positive Rate and False Positive Rate.\n",
    "AUC (Area Under the Curve) summarizes the overall performance - higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves and AUC for validation set\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_val, y_val_proba_knn)\n",
    "auc_knn = roc_auc_score(y_val, y_val_proba_knn)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_val, y_val_proba_rf)\n",
    "auc_rf = roc_auc_score(y_val, y_val_proba_rf)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {auc_knn:.4f})', linewidth=2)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nValidation Set AUC Scores:\")\n",
    "print(f\"  KNN:           {auc_knn:.4f}\")\n",
    "print(f\"  Random Forest: {auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 5-Fold Cross-Validation on Validation Set\n",
    "\n",
    "We'll perform 5-fold cross-validation on the validation set to get more robust estimates of AUC and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on validation set for both models\n",
    "print(\"Performing 5-fold cross-validation on validation set...\\n\")\n",
    "\n",
    "# KNN Cross-Validation\n",
    "knn_cv_scores = cross_validate(\n",
    "    knn_pipeline, X_val, y_val, \n",
    "    cv=cv, \n",
    "    scoring=['accuracy', 'roc_auc'],\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Random Forest Cross-Validation\n",
    "rf_cv_scores = cross_validate(\n",
    "    rf_pipeline, X_val, y_val, \n",
    "    cv=cv, \n",
    "    scoring=['accuracy', 'roc_auc'],\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Display results for each fold\n",
    "print(\"=\"*70)\n",
    "print(\"KNN - 5-Fold Cross-Validation Results (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Fold':<6} {'Accuracy':<12} {'AUC':<12}\")\n",
    "print(\"-\"*70)\n",
    "for i in range(5):\n",
    "    print(f\"{i+1:<6} {knn_cv_scores['test_accuracy'][i]:<12.4f} {knn_cv_scores['test_roc_auc'][i]:<12.4f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean':<6} {knn_cv_scores['test_accuracy'].mean():<12.4f} {knn_cv_scores['test_roc_auc'].mean():<12.4f}\")\n",
    "print(f\"{'Std':<6} {knn_cv_scores['test_accuracy'].std():<12.4f} {knn_cv_scores['test_roc_auc'].std():<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Random Forest - 5-Fold Cross-Validation Results (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Fold':<6} {'Accuracy':<12} {'AUC':<12}\")\n",
    "print(\"-\"*70)\n",
    "for i in range(5):\n",
    "    print(f\"{i+1:<6} {rf_cv_scores['test_accuracy'][i]:<12.4f} {rf_cv_scores['test_roc_auc'][i]:<12.4f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean':<6} {rf_cv_scores['test_accuracy'].mean():<12.4f} {rf_cv_scores['test_roc_auc'].mean():<12.4f}\")\n",
    "print(f\"{'Std':<6} {rf_cv_scores['test_accuracy'].std():<12.4f} {rf_cv_scores['test_roc_auc'].std():<12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "fold_nums = np.arange(1, 6)\n",
    "axes[0].plot(fold_nums, knn_cv_scores['test_accuracy'], marker='o', label='KNN', linewidth=2)\n",
    "axes[0].plot(fold_nums, rf_cv_scores['test_accuracy'], marker='s', label='Random Forest', linewidth=2)\n",
    "axes[0].axhline(knn_cv_scores['test_accuracy'].mean(), color='blue', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(rf_cv_scores['test_accuracy'].mean(), color='orange', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Fold Number', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('5-Fold CV: Accuracy per Fold', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(fold_nums)\n",
    "\n",
    "# AUC comparison\n",
    "axes[1].plot(fold_nums, knn_cv_scores['test_roc_auc'], marker='o', label='KNN', linewidth=2)\n",
    "axes[1].plot(fold_nums, rf_cv_scores['test_roc_auc'], marker='s', label='Random Forest', linewidth=2)\n",
    "axes[1].axhline(knn_cv_scores['test_roc_auc'].mean(), color='blue', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(rf_cv_scores['test_roc_auc'].mean(), color='orange', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Fold Number', fontsize=11)\n",
    "axes[1].set_ylabel('AUC', fontsize=11)\n",
    "axes[1].set_title('5-Fold CV: AUC per Fold', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(fold_nums)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Analysis\n",
    "\n",
    "### Binary Classification Task\n",
    "We created a binary classification problem by predicting whether a Spotify track is \"popular\" (popularity >= 50) or not, using the same audio features from Check-In 2.\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "#### Training Set Metrics\n",
    "Both KNN and Random Forest were evaluated on the training data with:\n",
    "- **Confusion Matrix**: Shows the breakdown of true positives, true negatives, false positives, and false negatives\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "- **Prediction Error**: 1 - Accuracy\n",
    "- **True Positive Rate (TPR)**: Proportion of actual positives correctly identified (Sensitivity/Recall)\n",
    "- **True Negative Rate (TNR)**: Proportion of actual negatives correctly identified (Specificity)\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "#### Validation Set Analysis\n",
    "- **ROC Curve**: Visualizes the trade-off between TPR and FPR at different classification thresholds\n",
    "- **AUC (Area Under Curve)**: Single metric summarizing overall classification performance\n",
    "  - AUC = 0.5: Random classifier\n",
    "  - AUC = 1.0: Perfect classifier\n",
    "  - AUC > 0.7: Generally considered acceptable\n",
    "  - AUC > 0.8: Good performance\n",
    "\n",
    "#### Cross-Validation Results\n",
    "5-fold cross-validation on the validation set provides:\n",
    "- More robust estimates of model performance\n",
    "- Insight into model stability across different data splits\n",
    "- Comparison of accuracy and AUC across folds\n",
    "\n",
    "### Key Observations\n",
    "- Random Forest typically performs better than KNN for this task due to its ability to capture non-linear relationships\n",
    "- The audio features provide moderate predictive power for popularity\n",
    "- Cross-validation helps ensure results are not due to a lucky train/validation split\n",
    "\n",
    "### Next Steps\n",
    "For the final project, consider:\n",
    "- Feature engineering (e.g., interaction terms, polynomial features)\n",
    "- Hyperparameter tuning (grid search for optimal k in KNN, tree depth in Random Forest)\n",
    "- Additional features (artist information, release year, genre)\n",
    "- Handling class imbalance if present\n",
    "- Ensemble methods combining multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
